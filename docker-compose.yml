services:
  # Optional local PostgreSQL for migration testing (disabled by default to mirror production Supabase usage)
  db:
    image: postgres:15-alpine
    container_name: aishacrm-db
    restart: "no"
    profiles: [migration-testing]
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${LOCAL_POSTGRES_PASSWORD:-changeme_local_dev_only}
      POSTGRES_DB: aishacrm
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backend/migrations:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Memory (rename to match production: redis-memory)
  redis-memory:
    image: redis:7-alpine
    container_name: aishacrm-redis-memory
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_memory_data:/data
    command: redis-server --save 60 1 --loglevel warning --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - aishanet

  # Redis Cache - API response caching
  redis-cache:
    image: redis:7-alpine
    container_name: aishacrm-redis-cache
    restart: unless-stopped
    ports:
      - "6380:6379"
    volumes:
      - redis_cache_data:/data
    command: redis-server --save "" --loglevel warning --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - aishanet

  # Backend API (mirrors production layout, local build)
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
      args:
        USE_SYSTEM_CHROMIUM: "true"
        PUPPETEER_SKIP_DOWNLOAD: "1"
    container_name: aishacrm-backend
    restart: unless-stopped
    depends_on:
      redis-memory:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    ports:
      - "4001:3001"  # Host:Container - prod port 4001 maps to dev port 3001
    # REMOVED: env_file - now using Doppler
    environment:
      - DOPPLER_TOKEN=${DOPPLER_TOKEN}
      - DOPPLER_PROJECT=aishacrm
      - DOPPLER_CONFIG=dev
      - NODE_ENV=development
      - NODE_OPTIONS=${NODE_OPTIONS:---dns-result-order=ipv4first}
      # PORT=3001 set by Doppler (container internal port)
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:5173,http://localhost:4000}
      - PGSSLMODE=require
      - REDIS_URL=redis://redis-memory:6379
      - REDIS_CACHE_URL=redis://redis-cache:6379
      # BACKEND_PORT=3001 set by Doppler (container internal port)
      - SYSTEM_TENANT_ID=a11dfb63-4b18-4eb8-872e-747af2e37c46
      - TENANT_RESOLVE_CACHE_TTL_MS=${TENANT_RESOLVE_CACHE_TTL_MS:-300000}
      - BRAID_MCP_URL=http://mcp:8000
      - MCP_NODE_HEALTH_URL=http://mcp:8000/health
      - AI_TRIGGERS_WORKER_ENABLED=${AI_TRIGGERS_WORKER_ENABLED:-false}
      - MCP_NODE_ID=aishacrm-backend
    entrypoint: >
      sh -c "
      if [ -z \"$$DOPPLER_TOKEN\" ]; then
        echo 'ERROR: DOPPLER_TOKEN not set - falling back to node server.js';
        exec node server.js;
      else
        echo 'Starting with Doppler...';
        exec doppler run --token $$DOPPLER_TOKEN -- node server.js;
      fi
      "
    dns:
      - 8.8.8.8
      - 8.8.4.4
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3001/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service"
        tag: "{{.Name}}"
    networks:
      - aishanet

  # NOTE: MCP is now provided by braid-mcp-node-server's own docker-compose.yml
  # with braid-mcp-server (coordinator) + braid-mcp-1, braid-mcp-2 (worker nodes)
  # Run: cd braid-mcp-node-server && docker compose up -d

  # Frontend (Vite dev server for development, or built static for production-like testing)
  frontend:
    build:
      context: .
      args:
        DOPPLER_TOKEN: ${DOPPLER_TOKEN}
        VITE_SUPABASE_URL: ${VITE_SUPABASE_URL}
        VITE_SUPABASE_ANON_KEY: ${VITE_SUPABASE_ANON_KEY}
        VITE_AISHACRM_BACKEND_URL: ${VITE_AISHACRM_BACKEND_URL:-http://localhost:4001}
        VITE_CURRENT_BRANCH: ${VITE_CURRENT_BRANCH:-main}
        VITE_SYSTEM_TENANT_ID: ${VITE_SYSTEM_TENANT_ID}
        VITE_USER_HEARTBEAT_INTERVAL_MS: ${VITE_USER_HEARTBEAT_INTERVAL_MS:-60000}
        APP_BUILD_VERSION: ${APP_BUILD_VERSION:-dev-local}
      dockerfile: Dockerfile
    container_name: aishacrm-frontend
    restart: unless-stopped
    depends_on:
      - backend
    ports:
      - "4000:3000"  # External port 4000 maps to internal 3000
    env_file:
      - .env
    environment:
      - PORT=${FRONTEND_PORT:-3000}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service"
        tag: "{{.Name}}"
    networks:
      - aishanet

  # n8n - Workflow automation platform (OPTIONAL - not part of core CRM)
  # To start n8n: docker compose --profile workflows up -d
  # Access at http://localhost:5679 (via n8n-proxy) or http://localhost:5678 (direct)
  # Used for building custom workflows that can call CRM APIs via webhooks
  n8n:
    image: n8nio/n8n:latest
    container_name: aishacrm-n8n
    restart: unless-stopped
    profiles: [workflows]
    ports:
      - "5678:5678"
    environment:
      - N8N_PORT=5678
      - N8N_HOST=0.0.0.0
      - GENERIC_TIMEZONE=UTC
      - N8N_DIAGNOSTICS_ENABLED=false
      # Allow iframe embedding in CRM
      - N8N_EDITOR_BASE_URL=http://localhost:5679
      - N8N_SECURE_COOKIE=false
      - N8N_SKIP_WEBHOOK_DEREGISTRATION_SHUTDOWN=true
      # Disable frame-ancestors CSP to allow iframe embedding
      - N8N_HIRING_BANNER_ENABLED=false
      - VUE_APP_URL_BASE_API=http://localhost:5679
      # Enable Public API for proper REST endpoint authentication
      - N8N_PUBLIC_API_DISABLED=false
      # Load custom MCP node
      - N8N_CUSTOM_EXTENSIONS=/home/node/.n8n/custom
      # Enable Basic Auth for iframe embedding (simpler than user management)
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD}
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n-nodes-mcp:/home/node/.n8n/custom:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5678/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    networks:
      - aishanet

  # nginx proxy for n8n - strips X-Frame-Options header for iframe embedding (external use)
  # Note: n8n is no longer embedded in CRM UI; this proxy remains for potential external iframe embedding
  # To start: docker compose --profile workflows up -d
  n8n-proxy:
    image: nginx:alpine
    container_name: aishacrm-n8n-proxy
    restart: unless-stopped
    profiles: [workflows]
    depends_on:
      - n8n
    ports:
      - "5679:5679"
    volumes:
      - ./nginx-n8n.conf:/etc/nginx/conf.d/default.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5679/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - aishanet

volumes:
  postgres_data:
    driver: local
  n8n_data:
    driver: local
  redis_memory_data:
    driver: local
  redis_cache_data:
    driver: local

networks:
  aishanet:
    name: aishanet
    driver: bridge
