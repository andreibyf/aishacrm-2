# Continue.dev Reranker Service

Improves code chat context quality by reranking semantic search results using BGE reranker.

## ğŸš€ Setup

### 1. Install Node.js dependencies (required for MCP server)

```bash
cd .continue/reranker
npm install
```

### 2. Start the Python reranker service

**First time** (downloads ~600MB model):
```batch
.continue\reranker\start-reranker.bat
```

Keep this terminal running. Service runs on `http://localhost:5001`

### 3. Reload VS Code

The MCP server is already configured in `.continue/mcpServers/new-mcp-server.yaml`

## ğŸ“Š Usage in Continue.dev

The reranker is now available as a tool in Continue.dev chat:

```
You: Find all React components that handle authentication

[Continue.dev does semantic search, then you can ask:]

You: Rerank these results to show the most relevant ones

[Continue.dev calls rerank_context tool automatically]
```

**Or manually trigger:**
```
@rerank query="authentication flow" in the top 10 results
```

## ğŸ§ª Test the Service

```bash
# Test reranker service directly
curl -X POST http://localhost:5001/rerank \
  -H "Content-Type: application/json" \
  -d '{
    "query": "React authentication",
    "documents": [
      "const LoginForm = () => { /* auth logic */ }",
      "const Dashboard = () => { /* dashboard */ }",
      "function authenticate(user) { /* auth */ }"
    ],
    "top_k": 2
  }'
```

## ğŸ“ˆ Performance

- **Model:** BAAI/bge-reranker-v2-m3 (~600MB)
- **Speed:** ~50ms for 10 documents
- **Context window:** 512 tokens per document
- **Accuracy:** +15-20% better context vs pure semantic search

## ğŸ”§ Configuration

Edit `.continue/reranker/service.py`:

```python
# Use faster/smaller model:
model = CrossEncoder('BAAI/bge-reranker-base')  # 278MB

# Or highest quality (slower):
model = CrossEncoder('BAAI/bge-reranker-large')  # 1.1GB
```

## ğŸ›‘ Stop Service

Press `Ctrl+C` in the reranker terminal, or:

```batch
taskkill //F //IM python.exe
```

## ğŸ“ How It Works

1. Continue.dev semantic search finds ~20 relevant code snippets
2. MCP server sends query + snippets to reranker service
3. BGE reranker scores each snippet against query
4. Top 5 most relevant results returned to LLM
5. LLM gets better context â†’ better answers

## âš ï¸ Troubleshooting

**"Model not loaded" error:**
- Wait 30s after starting service (model loads on first request)
- Check terminal for download progress

**MCP server not found:**
```bash
cd .continue/reranker
npm install
```

**Service won't start:**
```bash
# Check if port 5001 is busy
netstat -ano | findstr :5001
```
