# Aisha CRM Backend Server Configuration - PRODUCTION TEMPLATE
# Copy this to .env and fill in your actual values
# DO NOT commit .env files to git - they contain secrets!

# ========================================
# DOPPLER SECRETS MANAGEMENT (RECOMMENDED)
# ========================================
# Doppler tokens for secret management
# Dev token (read-write access to dev config)
DOPPLER_TOKEN=dp.st.dev.your_dev_token_here
# Production token (read-write access to production config)
DOPPLER_TOKEN_PRD=dp.st.prd_prd.your_production_token_here

# ========================================
# ENVIRONMENT
# ========================================
PORT=3001
NODE_ENV=development  # Set to 'production' in deployment

# Production Safety Guard - Allow writes to Supabase Cloud during dev/QA
# Set to 'false' or remove in production to prevent accidental mutations
ALLOW_PRODUCTION_WRITES=true

# ========================================
# JWT AUTHENTICATION (REQUIRED)
# ========================================
# Generate a strong secret with:
#   node -e "console.log(require('crypto').randomBytes(64).toString('hex'))"
# Minimum 32 characters required for production
JWT_SECRET=your_super_secret_jwt_key_change_this_to_random_64_chars

# ========================================
# SUPABASE CONFIGURATION (REQUIRED)
# ========================================
# Get these from your Supabase project settings → API
# PRODUCTION: https://ehjlenywplgyiahgxkfj.supabase.co
# DEV/QA: https://efzqxjpfewkrgpdootte.supabase.co

# Supabase Project URL
SUPABASE_URL=https://your-project.supabase.co

# Supabase Anonymous/Public Key (safe to expose in frontend)
SUPABASE_ANON_KEY=your_supabase_publishable_anon_key

# Supabase Service Role Key (SECRET - never expose or commit!)
# This key bypasses Row Level Security - backend use only
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_secret

# ========================================
# SUPABASE API MODE (RECOMMENDED)
# ========================================
# Set to true to use Supabase PostgREST API (HTTP) instead of direct PostgreSQL
# This avoids IPv6 connection issues with Docker
USE_SUPABASE_PROD=true
USE_SUPABASE_API=true

# ========================================
# DIRECT DATABASE CONNECTION (For performance logging)
# ========================================
# Direct PostgreSQL connection for performance_logs table
# DEV/QA: postgresql://postgres:Aml834VyYYH6humU@db.efzqxjpfewkrgpdootte.supabase.co:5432/postgres?sslmode=require
# PRODUCTION: postgresql://postgres:YOUR_PASSWORD@db.ehjlenywplgyiahgxkfj.supabase.co:5432/postgres?sslmode=require
DATABASE_URL=postgresql://postgres:YOUR_PASSWORD@db.YOUR_PROJECT_REF.supabase.co:5432/postgres?sslmode=require

# SSL Configuration
DB_SSL=true

# Performance Log Batching (optional tuning)
# Flush interval in milliseconds (default 2000 if unset)
PERF_LOG_FLUSH_MS=2000
# Max records per batch flush (default 25 if unset)
PERF_LOG_BATCH_MAX=25

# ========================================
# LEGACY DATABASE (Only if not using Supabase API)
# ========================================
# Leave commented out if USE_SUPABASE_PROD=true
# DATABASE_URL=postgresql://user:password@localhost:5432/aishacrm
# POSTGRES_HOST=localhost
# POSTGRES_PORT=5432
# POSTGRES_DB=aishacrm
# POSTGRES_USER=postgres
# POSTGRES_PASSWORD=your_password

# Local PostgreSQL Database (for offline development only)
# NOTE: Currently NOT used - backend connects to Supabase (USE_SUPABASE_PROD=true)
LOCAL_POSTGRES_PASSWORD=changeme_local_dev_only

# ========================================
# STORAGE
# ========================================
# Supabase Storage bucket for tenant assets (create in Supabase dashboard)
SUPABASE_STORAGE_BUCKET=tenant-assets

# ========================================
# CLOUDFLARE R2 ARTIFACT STORAGE (OPTIONAL)
# ========================================
# R2 is used for large AI-generated payloads (chat transcripts, agent traces, etc.)
# Keeps Postgres clean and enables scalable, immutable artifact storage
# Get these from Cloudflare Dashboard → R2 → Manage R2 API Tokens

# Cloudflare Account ID (found in R2 overview or account settings)
R2_ACCOUNT_ID=these_are_not_real_account_id_change_me

# R2 API Access Credentials (create via R2 → Manage R2 API Tokens)
R2_ACCESS_KEY_ID=these_are_not_real_account_id_change_me
R2_SECRET_ACCESS_KEY=these_are_not_real_account_id_change_me

# R2 Bucket Name (must be created in Cloudflare R2 dashboard first)
R2_BUCKET=aishacrm-artifacts

# R2 Endpoint (optional - auto-generated from account ID if not provided)
# R2_ENDPOINT=
# R2+TOKEN=

# AI Artifact Metadata Offload Threshold (bytes)
# If metadata JSON exceeds this size after tool results offload, the remaining metadata is also offloaded to R2
# Only a minimal envelope with pointer IDs is stored in Postgres
AI_ARTIFACT_META_THRESHOLD_BYTES=8000

# ========================================
# REDIS / VALKEY (OPTIONAL - AI memory layer)
# ========================================
# URL of Redis/Valkey instance. In Docker Compose this is typically redis://redis:6379
REDIS_URL=redis://localhost:6379

# ========================================
# USER MANAGEMENT
# ========================================
# DEPRECATED: Do not use a default password in production!
# Generate random passwords per user or use passwordless auth:
#   const tempPassword = crypto.randomBytes(16).toString('base64');
# Send via secure channel (email, SMS) with forced password reset
# DEFAULT_USER_PASSWORD=DEPRECATED_DO_NOT_USE

# ========================================
# SECURITY & MIDDLEWARE (REQUIRED FOR PRODUCTION)
# ========================================
# JWT Token Expiration
JWT_EXPIRES_IN=24h

# Rate Limiting
# For E2E tests: Very high limits to prevent 429 errors during parallel test execution
RATE_LIMIT_WINDOW_MS=60000  # 1 minute window
RATE_LIMIT_MAX=100000  # 100000 requests per window (for E2E testing with 12 parallel workers)
RATE_LIMIT_TEST_MAX=10000  # Effective cap in test/E2E mode for triggering 429
RATE_LIMIT_FORCE_DEFAULT=1  # Force using test max even if high RATE_LIMIT_MAX present

# E2E Test Mode
# Enable test-mode behaviors (rate limit override, test email visibility suppression disabled)
E2E_TEST_MODE=true

# CORS - REQUIRED FOR PRODUCTION
# Comma-separated list of allowed origins (NO SPACES)
# Example: https://app.aishacrm.com,https://www.aishacrm.com
# Development: http://localhost:4000,http://localhost:5173
# CRITICAL: In production, remove localhost origins and add your actual domain
ALLOWED_ORIGINS=http://localhost:4000,http://localhost:5173

# Frontend URL - REQUIRED FOR PRODUCTION
# Used for password reset emails, user invitations, and OAuth redirects
# Must match your frontend domain exactly
# Example: https://app.aishacrm.com
# CRITICAL: If not set, password reset will fail in production
FRONTEND_URL=http://localhost:4000

# ========================================
# INTRUSION DETECTION & RESPONSE (IDR)
# ========================================
# IP Whitelist - Trusted IPs that bypass ALL IDR checks (prevents admin lockouts)
# Comma-separated list of IPs/CIDR blocks (no spaces)
# Default: localhost + Docker internal networks
IDR_WHITELIST_IPS=127.0.0.1,::1,172.16.0.0/12,192.168.0.0/16,10.0.0.0/8

# Emergency Unblock Secret - Required for /api/security/emergency-unblock endpoint
# Generate with: node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"
# Keep this secret safe - it allows unblocking IPs without authentication
IDR_EMERGENCY_SECRET=emergency_unblock_secret_2024

# External Threat Intelligence (Free Tier APIs)
# AbuseIPDB API Key (free: 1000 checks/day) - Get from https://www.abuseipdb.com/account/api
# Leave empty to disable external threat intel
ABUSEIPDB_API_KEY=

# ========================================
# DATA MIGRATION (OPTIONAL - Base44 sync only)
# ========================================
#BASE44_APP_ID=68f83fa997417472c872be53
#BASE44_API_KEY=your_base44_api_key

# ========================================
# WORKFLOW AUTOMATION (OPTIONAL)
# ========================================
# n8n Integration
# Base URL of your n8n instance (e.g., http://localhost:5678 or http://n8n:5678 if on same docker network)
N8N_BASE_URL=http://localhost:5678
# Optional API key if Public API is enabled in n8n
N8N_API_KEY=your_n8n_api_key_here
# Optional Basic Auth for legacy /rest endpoints (enable in n8n container)
N8N_BASIC_AUTH_USER=
N8N_BASIC_AUTH_PASSWORD=

# ========================================
# AI CAMPAIGN WORKER (OPTIONAL)
# ========================================
# Enable background campaign execution worker
# Set to 'true' to process scheduled campaigns automatically
CAMPAIGN_WORKER_ENABLED=false

# Campaign polling interval in milliseconds
# Default: 30000 (30 seconds)
# Recommended: 30000-60000 depending on urgency requirements
CAMPAIGN_WORKER_INTERVAL_MS=30000

# AI Triggers Worker (Background AI processing)
AI_TRIGGERS_WORKER_ENABLED=true
AI_TRIGGERS_WORKER_INTERVAL_MS=15000

# ========================================
# CALL FLOW & TRANSCRIPT ANALYSIS
# ========================================
# Use Braid MCP Server for AI-powered transcript analysis
# Set to 'true' to enable OpenAI-powered action item extraction
# Requires braid-mcp-node-server running (cd braid-mcp-node-server && docker compose up -d --build)
USE_BRAID_MCP_TRANSCRIPT_ANALYSIS=true

# Braid MCP Server URL
# Default: http://braid-mcp-node-server:8000 (Docker Compose)
# Local dev: http://localhost:8000
BRAID_MCP_URL=http://braid-mcp-node-server:8000

# MCP Node Health Check URL (for System Health dashboard)
MCP_NODE_HEALTH_URL=http://braid-mcp-node-server:8000/health

# OpenAI model for transcript analysis
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Default: gpt-4o-mini (fast, cost-effective for call transcripts)
TRANSCRIPT_ANALYSIS_MODEL=gpt-4o-mini

# ========================================
# WEBHOOKS (OPTIONAL)
# ========================================
# Enable tenant-scoped webhook emissions for lifecycle events
# Set to 'true' to emit webhooks for campaign events, entity changes, etc.
WEBHOOKS_ENABLED=false

# ========================================
# AI SERVICES (OPTIONAL)
# ========================================

# ----------------------------------------
# LLM Provider Configuration
# ----------------------------------------
# Primary LLM provider: openai | anthropic | groq | local
LLM_PROVIDER=openai

# Failover chain (comma-separated, tries in order if primary fails)
LLM_FAILOVER_CHAIN=openai,anthropic,groq

# ----------------------------------------
# OpenAI (default provider)
# ----------------------------------------
OPENAI_API_KEY=your_openai_api_key
# OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: override base URL

# ----------------------------------------
# Anthropic (Claude) - Optional alternative provider
# ----------------------------------------
# ANTHROPIC_API_KEY=sk-ant-your_anthropic_api_key

# ----------------------------------------
# Groq - Optional alternative provider (fast, cost-effective)
# ----------------------------------------
# GROQ_API_KEY=gsk_your_groq_api_key
# GROQ_BASE_URL=https://api.groq.com/openai/v1

# ----------------------------------------
# Local LLM - Optional self-hosted (LM Studio, vLLM, etc.)
# ----------------------------------------
# LOCAL_LLM_BASE_URL=http://localhost:1234/v1
# LOCAL_LLM_API_KEY=lm-studio

# ----------------------------------------
# Model Overrides (Optional - uses sensible defaults)
# ----------------------------------------
# Override specific capabilities per provider:
# OPENAI_MODEL_CHAT_TOOLS=gpt-4o
# OPENAI_MODEL_JSON_STRICT=gpt-4o-mini
# ANTHROPIC_MODEL_CHAT_TOOLS=claude-3-5-sonnet-20241022
# ANTHROPIC_MODEL_JSON_STRICT=claude-3-haiku-20240307
# GROQ_MODEL_CHAT_TOOLS=llama-3.3-70b-versatile
# GROQ_MODEL_JSON_STRICT=llama-3.1-8b-instant

# Default AI Model for Text Chat
# Model used by /api/ai/chat endpoint for conversational AI.
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Default: gpt-4o (best quality, recommended for CRM insights)
# Cost: ~$2.50/1M input tokens, $10/1M output tokens
DEFAULT_OPENAI_MODEL=gpt-4o

# Realtime Voice (Phase 2C)
# Model for OpenAI Realtime WebRTC audio sessions. Must use realtime-preview model.
# Default: gpt-4o-realtime-preview-2024-12-17
# Cost: ~$0.06/minute (charged by audio time, not tokens)
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview-2024-12-17
OPENAI_REALTIME_VOICE=marin

# Speech-to-Text (Phase 2C)
# Model used by /api/ai/speech-to-text endpoint when transcribing audio uploads.
# Default: whisper-1 (OpenAI Whisper). Override if you have access to gpt-4o-mini-transcribe.
OPENAI_STT_MODEL=whisper-1

# Maximum size (in bytes) for uploaded audio clips. Default 6 MB (about ~30 seconds of speech in webm format).
MAX_STT_AUDIO_BYTES=6000000

# ElevenLabs TTS (Phase 2C)
# API Key and Voice ID used by /api/ai/tts endpoint to generate assistant speech.
# Obtain from https://elevenlabs.io/ and set a voice for your assistant.
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=

# AI Brain Testing (Phase 1)
# Secret key required for /api/ai/brain-test endpoint
# Generate with: openssl rand -hex 32
INTERNAL_AI_TEST_KEY=your_secret_key_here_min_32_chars

# ========================================
# FEATURE FLAGS
# ========================================
# Enable V2 Opportunities API usage
FEATURE_OPPORTUNITIES_V2=true
# Enable AI Assistant sidebar
FEATURE_FLAG_AI_ASSISTANT=true
# Enable enhanced accessibility features
FEATURE_FLAG_ENHANCED_A11Y=true
# Enable business hours tracking
FEATURE_FLAG_BUSINESS_HOURS=true

# ========================================
# AI MEMORY (RAG) - PHASE 7
# ========================================
# Enable AI memory system for context-aware conversations
# Stores embeddings of notes and activities for retrieval
MEMORY_ENABLED=true

# Number of memory chunks to retrieve for context (topK)
# Higher values = more context, slower retrieval
# Recommended: 5-10 for balanced performance
MEMORY_TOP_K=8

# Maximum characters per memory chunk
# Smaller chunks = more precise retrieval but more chunks needed
# Larger chunks = more context per chunk but less precise
# Recommended: 2500-4000 (fits within most embedding limits)
MEMORY_MAX_CHUNK_CHARS=3500

# Minimum similarity threshold for memory retrieval (0-1)
# Higher values = only very relevant memories
# Lower values = more memories but potentially less relevant
# Recommended: 0.6-0.8
MEMORY_MIN_SIMILARITY=0.7

# Embedding provider for memory vectors
# Options: openai (default), anthropic, local
# Must match one of your configured LLM providers
MEMORY_EMBEDDING_PROVIDER=openai

# Embedding model for generating vectors
# OpenAI: text-embedding-3-small (1536 dims, $0.02/1M tokens)
#         text-embedding-3-large (3072 dims, $0.13/1M tokens)
# Must match vector dimension in migration (default 1536)
MEMORY_EMBEDDING_MODEL=text-embedding-3-small

# ========================================
# TENANT RESOLUTION & ARCHIVAL
# ========================================
# SYSTEM_TENANT_ID: Canonical UUID for internal 'system' tenant slug
# Used by archival jobs and system-level operations
SYSTEM_TENANT_ID=a11dfb63-4b18-4eb8-872e-747af2e37c46

# TENANT_RESOLVE_CACHE_TTL_MS: Cache TTL for tenant identity resolution (milliseconds)
# Default: 60000 (60 seconds)
# Recommended for production: 300000 (5 minutes) - better cache efficiency
# Adjust based on tenant schema change frequency:
#   - High change frequency: 30000-60000 (30s-1min)
#   - Medium change frequency: 120000-300000 (2-5min)
#   - Low change frequency: 300000-600000 (5-10min)
TENANT_RESOLVE_CACHE_TTL_MS=300000

# ========================================
# REDIS CONFIGURATION (REQUIRED)
# ========================================
# Redis Memory (ephemeral: AI sessions, presence, real-time)
REDIS_MEMORY_URL=redis://localhost:6379
# Redis Cache (persistent: API responses, dashboard stats, aggregations)
REDIS_CACHE_URL=redis://localhost:6380

# ========================================
# SERVER HEALTH & MONITORING
# ========================================
# Heartbeat interval (ms) for backend server health logging
HEARTBEAT_INTERVAL_MS=120000
HEARTBEAT_ENABLED=true

# ========================================
# PAYMENT & BILLING (OPTIONAL - Stripe)
# ========================================
STRIPE_SECRET_KEY=sk_test_your_stripe_secret_key
STRIPE_WEBHOOK_SECRET=whsec_your_webhook_signing_secret

# ========================================
# TELEPHONY (OPTIONAL)
# ========================================
# Twilio (global fallback – per-tenant credentials go in tenant_integrations table)
TWILIO_ACCOUNT_SID=your_twilio_sid
TWILIO_AUTH_TOKEN=your_twilio_token
TWILIO_FROM_NUMBER=+15551234567

# SignalWire
SIGNALWIRE_PROJECT_ID=your_signalwire_project
SIGNALWIRE_API_TOKEN=your_signalwire_token

# ========================================
# MICROSOFT INTEGRATION (OPTIONAL)
# ========================================
# Microsoft Graph API for Office 365, Outlook, Teams
MICROSOFT_CLIENT_ID=your_client_id
MICROSOFT_CLIENT_SECRET=your_client_secret
MICROSOFT_TENANT_ID=your_tenant_id

# ========================================
# GOOGLE INTEGRATION (OPTIONAL)
# ========================================
# Google Workspace, Calendar, Gmail
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret

# ========================================
# ALTERNATIVE STORAGE (OPTIONAL)
# ========================================
# MinIO (S3-compatible)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_USE_SSL=false

# Cloudflare R2 (S3-compatible)
R2_ACCOUNT_ID=your_cloudflare_account_id
R2_ACCESS_KEY_ID=your_r2_access_key
R2_SECRET_ACCESS_KEY=your_r2_secret_key

# ========================================
# GITHUB MODEL CONTEXT PROTOCOL (OPTIONAL)
# ========================================
# If set, backend will expose a 'GitHub MCP' server at /api/mcp/servers
GITHUB_TOKEN=ghp_your_token_here

# ========================================
# CI / E2E INTEGRATION (OPTIONAL)
# ========================================
# Backend can trigger GitHub Actions workflow to run Playwright E2E tests
# POST /api/testing/run-playwright with { suite: "metrics|rls|rate-limit|notifications|tenant|crud|all" }
# These values default to this repo but can be customized
# GitHub repository config (renamed for GitHub Actions compatibility)
# New names: REPO_OWNER, REPO_NAME, WORKFLOW_FILE, GH_TOKEN
# Old names still supported as fallbacks
GITHUB_TOKEN=github_pat_your_token_here
GITHUB_REPO_OWNER=andreibyf
GITHUB_REPO_NAME=aishacrm-2
GITHUB_WORKFLOW_FILE=e2e.yml
REPO_OWNER=andreibyf
REPO_NAME=aishacrm-2
WORKFLOW_FILE=e2e.yml
GH_TOKEN=github_pat_your_token_here

# ========================================
# TEST ACCOUNT (FOR AUTOMATED SCRIPTS)
# ========================================
# Test account password (optional, for automated script)
TEST_PASSWORD="DevTest#002"





# ========================================
# PRODUCTION DEPLOYMENT CHECKLIST
# ========================================
# Before deploying to production:
#
# 1. ROW LEVEL SECURITY (RLS):
#    - Apply backend/migrations/999_enable_rls_policies.sql to your Supabase database
#    - Verify with: SELECT tablename, rowsecurity FROM pg_tables WHERE schemaname='public';
#    - Test: Direct anon key access should return no rows; backend API should work normally
#
# 2. SECRETS MANAGEMENT:
#    - Generate new JWT_SECRET: node -e "console.log(require('crypto').randomBytes(64).toString('hex'))"
#    - Rotate all Supabase keys (create new service_role key in dashboard)
#    - Use platform secret management (Railway Secrets, Vercel Environment Variables)
#    - NEVER commit .env files to git
#
# 3. ENVIRONMENT VARIABLES:
#    - Set NODE_ENV=production
#    - Update ALLOWED_ORIGINS to production domain(s)
#    - Remove DEFAULT_USER_PASSWORD or use secure random generation
#
# 4. DATABASE:
#    - Run all migrations in backend/migrations/ in order
#    - Verify RLS policies are enabled and working
#    - Set up automated backups in Supabase dashboard
#
# 5. MONITORING:
#    - Enable Supabase logging and monitoring
#    - Set up error tracking (Sentry, LogRocket, etc.)
#    - Configure health check endpoint monitoring
#
# 6. SECURITY MIDDLEWARE:
#    - Verify helmet.js is configured (HTTP security headers)
#    - Confirm rate limiting is active
#    - Test CORS configuration with production frontend
#
# 7. SECRET ROTATION:
#    - Plan rotation schedule for JWT_SECRET, API keys, database credentials
#    - Document rotation procedures for team
#    - Test rotation process in staging before production
