# Aisha CRM Database Administration Manual - Part 2

**Version:** 1.0  
**Last Updated:** November 16, 2025  
**Audience:** Database Administrators, DevOps Engineers, System Administrators

---

## Table of Contents

### Part 2: Operations & Maintenance

**[‚Üê Return to Part 1](./AISHA_CRM_DATABASE_MANUAL_PART1.md)** for Chapters 1-6

#### Chapter 7: Comprehensive Migration History
- [7.1 Initial Schema (001-010)](#71-initial-schema-001-010)
- [7.2 Security & RLS (011-024)](#72-security--rls-011-024)
- [7.3 Features & Optimization (025-036)](#73-features--optimization-025-036)
- [7.4 Major Refactoring (037-052)](#74-major-refactoring-037-052)
- [7.5 Special Migrations](#75-special-migrations)

#### Chapter 8: Backup & Recovery
- [8.1 Backup Strategy](#81-backup-strategy)
- [8.2 Automated Backups](#82-automated-backups)
- [8.3 Manual Backups](#83-manual-backups)
- [8.4 Point-in-Time Recovery](#84-point-in-time-recovery)
- [8.5 Disaster Recovery](#85-disaster-recovery)

#### Chapter 9: Performance Monitoring
- [9.1 Supabase Dashboard](#91-supabase-dashboard)
- [9.2 Query Performance](#92-query-performance)
- [9.3 Connection Pooling](#93-connection-pooling)
- [9.4 Slow Query Detection](#94-slow-query-detection)
- [9.5 Performance Optimization](#95-performance-optimization)

#### Chapter 10: Maintenance Tasks
- [10.1 Routine Maintenance](#101-routine-maintenance)
- [10.2 Vacuum & Analyze](#102-vacuum--analyze)
- [10.3 Index Maintenance](#103-index-maintenance)
- [10.4 Table Bloat](#104-table-bloat)
- [10.5 Statistics Updates](#105-statistics-updates)

#### Chapter 11: Troubleshooting
- [11.1 Common Errors](#111-common-errors)
- [11.2 Connection Issues](#112-connection-issues)
- [11.3 Migration Failures](#113-migration-failures)
- [11.4 RLS Debugging](#114-rls-debugging)
- [11.5 Data Inconsistencies](#115-data-inconsistencies)

#### Chapter 12: Best Practices
- [12.1 Development Workflow](#121-development-workflow)
- [12.2 Testing Migrations](#122-testing-migrations)
- [12.3 Documentation Standards](#123-documentation-standards)
- [12.4 Security Considerations](#124-security-considerations)
- [12.5 Scaling Strategies](#125-scaling-strategies)

#### Appendices
- [Appendix A: Complete Table Reference](#appendix-a-complete-table-reference)
- [Appendix B: Migration Quick Reference](#appendix-b-migration-quick-reference)
- [Appendix C: Supabase Features](#appendix-c-supabase-features)
- [Appendix D: SQL Query Examples](#appendix-d-sql-query-examples)
- [Appendix E: Troubleshooting Flowchart](#appendix-e-troubleshooting-flowchart)

---

## Chapter 7: Comprehensive Migration History

### 7.1 Initial Schema (001-010)

#### 001_init.sql
**Date:** 2025-01-15  
**Purpose:** Create core database schema with essential tables

**Tables Created:**
- `users` - Global user accounts (superadmins)
- `accounts` - Companies/organizations
- `contacts` - Individual people
- `leads` - Potential customers
- `activities` - Tasks, calls, emails, meetings
- `opportunities` - Sales deals
- `notifications` - User notifications
- `system_logs` - Application logging
- `employees` - Tenant-scoped users

**Key Features:**
- UUID primary keys with `gen_random_uuid()`
- JSONB metadata columns for flexibility
- Tenant isolation via `tenant_id`
- Timestamptz for temporal data
- Foreign key relationships

**Example:**
```sql
CREATE EXTENSION IF NOT EXISTS pgcrypto;

CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255),
  first_name VARCHAR(255),
  last_name VARCHAR(255),
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE accounts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id TEXT NOT NULL,
  name VARCHAR(255) NOT NULL,
  industry VARCHAR(255),
  website VARCHAR(255),
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

#### 002_seed.sql
**Date:** 2025-01-15  
**Purpose:** Seed initial data for development and testing

**Data Inserted:**
- Test superadmin user
- Sample tenant
- Demo accounts, contacts, leads
- Initial activities and opportunities

---

#### 003_create_apikey.sql
**Date:** 2025-01-20  
**Purpose:** Add API key management for external integrations

**Tables Created:**
- `api_key` - API authentication tokens

**Features:**
- bcrypt-hashed keys
- Tenant-scoped keys
- Expiration support
- Rate limiting metadata

```sql
CREATE TABLE api_key (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL,
  name VARCHAR(255) NOT NULL,
  key_hash VARCHAR(255) NOT NULL,
  expires_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

#### 004_tenant_integrations.sql
**Date:** 2025-01-25  
**Purpose:** Support third-party integrations per tenant

**Tables Created:**
- `tenant_integrations` - OAuth credentials, API keys, webhook URLs

---

#### 005_bizdev_sources.sql
**Date:** 2025-02-01  
**Purpose:** Track business development lead sources

**Tables Created:**
- `bizdev_sources` - Lead generation channels and attribution

---

#### 006_tenant_table.sql
**Date:** 2025-02-05  
**Purpose:** Formalize multi-tenant architecture

**Tables Created:**
- `tenant` - Organization/company records with plan, status, metadata

**Changes:**
- Added `tenant` as formal entity
- Updated foreign keys to reference `tenant(id)`

---

#### 007_crud_enhancements.sql
**Date:** 2025-02-10  
**Purpose:** Add missing fields for CRUD operations

**Changes:**
- Added `updated_at` to multiple tables
- Added `deleted_at` for soft deletes (some tables)
- Added owner/assignee fields

---

#### 008_rls_policies.sql
**Date:** 2025-02-15  
**Purpose:** Initial row-level security implementation

**Features:**
- Enabled RLS on core tables
- Basic tenant isolation policies
- Service role bypass

```sql
ALTER TABLE accounts ENABLE ROW LEVEL SECURITY;

CREATE POLICY "tenant_isolation" ON accounts
  FOR ALL USING (tenant_id = current_setting('app.current_tenant_id', true)::text);
```

---

#### 009_complete_schema.sql
**Date:** 2025-02-20  
**Purpose:** Add remaining domain tables

**Tables Created:**
- `documents` - File storage metadata
- `workflows` - Automation definitions
- `permissions` - Role-based access control

---

#### 010_multiple_additions.sql
**Date:** 2025-03-01  
**Purpose:** Three separate additions merged into one migration

**Changes:**
- Added `revenue` column to `accounts`
- Added `created_date` to `leads`
- Added `role` to `users` table

---

### 7.2 Security & RLS (011-024)

#### 011_enable_rls.sql
**Date:** 2025-03-05  
**Purpose:** Enable RLS on all tenant-scoped tables

**Tables Affected:** 25+ tables

---

#### 012_extend_activities_fields.sql
**Date:** 2025-03-10  
**Purpose:** Enhance activities table with additional fields

**Changes:**
- Added `priority`, `status`, `due_date`
- Added `assigned_to` foreign key

---

#### 012_add_tenant_id_to_users.sql
**Date:** 2025-03-10  
**Purpose:** Link users to tenants (employees only)

---

#### 013_allow_null_tenant_id.sql
**Date:** 2025-03-15  
**Purpose:** Support global users (superadmins) without tenant

**Changes:**
- Made `tenant_id` nullable on `users` table
- Updated RLS policies to handle NULL

---

#### 013_tighten_data_types.sql
**Date:** 2025-03-15  
**Purpose:** Enforce stricter data types for data integrity

**Changes:**
- Changed `VARCHAR` lengths to appropriate sizes
- Added `CHECK` constraints
- Changed `TEXT` to `VARCHAR(255)` where appropriate

---

#### 014_conversations.sql
**Date:** 2025-03-20  
**Purpose:** Add AI conversation tracking

**Tables Created:**
- `conversations` - AI chat threads
- `conversation_messages` - Individual messages

```sql
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL,
  user_email VARCHAR(255),
  status VARCHAR(50) DEFAULT 'active',
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

#### 015_add_job_title_to_leads.sql
**Date:** 2025-03-25  
**Purpose:** Capture job title for lead qualification

---

#### 019_cleanup_duplicate_emails.sql
**Date:** 2025-04-10  
**Purpose:** Remove duplicate emails before enforcing uniqueness

**Process:**
1. Identify duplicates across `users` and `employees`
2. Keep oldest record, delete others
3. Log deletions to audit_log

---

#### 020_enforce_email_uniqueness.sql
**Date:** 2025-04-15  
**Purpose:** Prevent duplicate emails across users and employees

**Implementation:**
1. **Database Constraint:** UNIQUE index on email (case-insensitive)
2. **Trigger:** Cross-table email check
3. **Backend Validation:** Explicit checks before INSERT/UPDATE

```sql
-- Unique constraint
CREATE UNIQUE INDEX users_email_unique ON users (LOWER(email));
CREATE UNIQUE INDEX employees_email_unique ON employees (LOWER(email), tenant_id);

-- Trigger function
CREATE OR REPLACE FUNCTION check_email_uniqueness()
RETURNS TRIGGER AS $$
BEGIN
  IF EXISTS (SELECT 1 FROM users WHERE LOWER(email) = LOWER(NEW.email)) THEN
    RAISE EXCEPTION 'Email already exists in users table';
  END IF;
  IF EXISTS (SELECT 1 FROM employees WHERE LOWER(email) = LOWER(NEW.email)) THEN
    RAISE EXCEPTION 'Email already exists in employees table';
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

---

#### 020_performance_logs.sql
**Date:** 2025-04-20  
**Purpose:** Track API endpoint performance

**Tables Created:**
- `performance_logs` - Request timing, status codes, errors

---

#### 021_make_email_optional.sql
**Date:** 2025-04-25  
**Purpose:** Allow contacts without email addresses

**Changes:**
- Made `email` nullable on `contacts` table
- Updated validation to allow NULL

---

#### 021_systembranding.sql
**Date:** 2025-04-25  
**Purpose:** White-label customization per tenant

**Tables Created:**
- `systembranding` - Logo, colors, custom domain

---

#### 022_secure_performance_logs.sql
**Date:** 2025-05-01  
**Purpose:** Apply RLS to performance_logs

---

#### 023_comprehensive_rls_security.sql
**Date:** 2025-11-05  
**Purpose:** Complete RLS implementation across all 48+ tables

**Impact:** üö® **CRITICAL MIGRATION** - Enforces tenant isolation at database level

**Features:**
- Enabled RLS on 48 tables
- Tenant isolation policies
- Service role bypass for backend
- Superadmin access policies

**Example Policy:**
```sql
-- Tenant isolation
CREATE POLICY "tenant_isolation" ON contacts
  FOR ALL USING (
    tenant_id = current_setting('app.current_tenant_id', true)::uuid
    OR
    EXISTS (
      SELECT 1 FROM users
      WHERE email = current_user
      AND role = 'superadmin'
    )
  );

-- Service role bypass
ALTER TABLE contacts FORCE ROW LEVEL SECURITY;
```

---

#### 024_fix_function_search_path.sql
**Date:** 2025-11-06  
**Purpose:** Fix security vulnerability in PostgreSQL functions

**Problem:** Functions with `SECURITY DEFINER` and mutable `search_path` are vulnerable

**Solution:** Set explicit schema-qualified function calls

**Functions Fixed:**
- `check_email_uniqueness()`
- `update_updated_at()`
- `log_audit_changes()`
- `validate_tenant_access()`

```sql
ALTER FUNCTION check_email_uniqueness() SET search_path = public;
ALTER FUNCTION update_updated_at() SET search_path = public;
```

---

### 7.3 Features & Optimization (025-036)

#### 025_add_foreign_key_indexes.sql
**Date:** 2025-11-07  
**Purpose:** Improve JOIN performance and foreign key checks

**Indexes Created:** 30+ foreign key indexes

```sql
CREATE INDEX IF NOT EXISTS idx_contacts_account_id ON contacts(account_id);
CREATE INDEX IF NOT EXISTS idx_contacts_owner_id ON contacts(owner_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_account_id ON opportunities(account_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_contact_id ON opportunities(contact_id);
CREATE INDEX IF NOT EXISTS idx_activities_related_id ON activities(related_id);
-- ... 25 more indexes
```

**Performance Impact:**
- JOIN queries 5-10x faster
- Foreign key constraint checks 3x faster

---

#### 025_synchealth.sql
**Date:** 2025-11-07  
**Purpose:** Monitor integration sync status

**Tables Created:**
- `sync_health` - Integration sync status, last success, error tracking

---

#### 026_create_test_user.sql
**Date:** 2025-11-08  
**Purpose:** Create standardized test user for development

---

#### 027_create_superadmin_user.sql
**Date:** 2025-11-08  
**Purpose:** Create initial superadmin account

**User Created:**
- Email: admin@aishacrm.com
- Role: superadmin
- Password: (hashed, set during migration)

---

#### 028_fix_rls_apikey_systembranding.sql
**Date:** 2025-11-08  
**Purpose:** Fix missing RLS policies on api_key and systembranding

---

#### 029_enforce_account_name_uniqueness.sql
**Date:** 2025-11-09  
**Purpose:** Prevent duplicate account names per tenant

```sql
CREATE UNIQUE INDEX accounts_name_tenant_unique 
  ON accounts(LOWER(name), tenant_id);
```

---

#### 030_update_port_references.sql
**Date:** 2025-11-09  
**Purpose:** Update Docker port references (4000/4001)

**Changes:**
- Updated `FRONTEND_URL` in tenant metadata
- Updated webhook URLs in integrations

---

#### 031_create_ai_campaigns.sql
**Date:** 2025-11-09  
**Purpose:** AI-powered marketing campaigns

**Tables Created:**
- `ai_campaigns` - Campaign definitions
- `campaign_recipients` - Target list
- `campaign_analytics` - Performance metrics

---

#### 031_rename_cash_flow_type_to_transaction_type.sql
**Date:** 2025-11-09  
**Purpose:** Clarify column naming in cash_flow table

---

#### 032_normalize_foreign_keys.sql
**Date:** 2025-11-10  
**Purpose:** Standardize foreign key behavior

**Changes:**
- Updated ON DELETE actions to consistent values
- Added missing foreign keys
- Removed orphaned records

---

#### 033_entity_lifecycle_with_id_preservation.sql
**Date:** 2025-11-10  
**Purpose:** Support lead ‚Üí contact ‚Üí opportunity conversion with ID preservation

**Tables Created:**
- `entity_transitions` - Track conversions between entity types

**Features:**
- Preserve UUID when converting lead to contact
- Maintain history of entity transformations
- Support rollback to previous entity type

```sql
CREATE TABLE entity_transitions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL,
  entity_id UUID NOT NULL,
  from_type VARCHAR(50) NOT NULL,
  to_type VARCHAR(50) NOT NULL,
  transition_data JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

#### 034_consolidate_tenant_integrations.sql
**Date:** 2025-11-10  
**Purpose:** Merge multiple integration tables into one

**Impact:** Simplified schema, reduced joins

---

#### 035_consolidate_ai_campaigns.sql
**Date:** 2025-11-10  
**Purpose:** Merge campaign tables into single table with JSONB

---

#### 036_cleanup_ai_campaign_residue.sql
**Date:** 2025-11-11  
**Purpose:** Remove obsolete campaign-related tables and columns

---

### 7.4 Major Refactoring (037-052)

#### 037_add_conversation_title_topic.sql
**Date:** 2025-11-10  
**Purpose:** Enhance conversations with title and topic fields

**Changes:**
```sql
ALTER TABLE conversations ADD COLUMN title VARCHAR(255);
ALTER TABLE conversations ADD COLUMN topic VARCHAR(255);

CREATE INDEX idx_conversations_topic ON conversations(topic);
```

---

#### 038_add_tenant_uuid_to_users.sql
**Date:** 2025-11-11  
**Purpose:** Migrate tenant references from slug to UUID

**Process:**
1. Add `tenant_uuid` column to `users` table
2. Populate from `tenant(slug ‚Üí id)` mapping
3. Create foreign key to `tenant(id)`

---

#### 039_align_ui_schema.sql
**Date:** 2025-11-11  
**Purpose:** Ensure database schema matches UI expectations

**Changes:**
- Added missing columns shown in UI
- Renamed columns for consistency
- Updated default values

---

#### 040_moves_and_views.sql
**Date:** 2025-11-11  
**Purpose:** Create database views for common queries

**Views Created:**
- `v_contact_full` - Contacts with account info
- `v_opportunity_pipeline` - Sales pipeline summary
- `v_activity_calendar` - Calendar view of activities

```sql
CREATE OR REPLACE VIEW v_contact_full AS
SELECT 
  c.*,
  a.name AS account_name,
  a.industry AS account_industry,
  e.first_name || ' ' || e.last_name AS owner_name
FROM contacts c
LEFT JOIN accounts a ON c.account_id = a.id
LEFT JOIN employees e ON c.owner_id = e.id;
```

---

#### 041_calendar_view_metadata_fallback.sql
**Date:** 2025-11-11  
**Purpose:** Support legacy metadata-based calendar events

---

#### 042_fix_entity_transitions_tenant_id_type.sql
**Date:** 2025-11-11  
**Purpose:** Change `tenant_id` from TEXT to UUID in entity_transitions

```sql
ALTER TABLE entity_transitions 
  ALTER COLUMN tenant_id TYPE UUID USING tenant_id::uuid;
```

---

#### 050_flatten_tenant_metadata.sql
**Date:** 2025-11-12  
**Purpose:** Move critical tenant fields from metadata JSONB to columns

**Changes:**
```sql
ALTER TABLE tenant ADD COLUMN plan VARCHAR(50);
ALTER TABLE tenant ADD COLUMN status VARCHAR(50);
ALTER TABLE tenant ADD COLUMN settings JSONB DEFAULT '{}';

-- Migrate data
UPDATE tenant SET
  plan = metadata->>'plan',
  status = metadata->>'status',
  settings = metadata->'settings';

-- Remove from metadata
UPDATE tenant SET metadata = metadata - 'plan' - 'status' - 'settings';
```

**Benefits:**
- Faster queries (no JSONB parsing)
- Better indexing
- Type safety

---

#### 051_fix_table_name_consistency.sql
**Date:** 2025-11-13  
**Purpose:** Standardize table naming (singular vs plural)

**Changes:**
- Renamed `employees` ‚Üí `employee`
- Renamed `bizdev_sources` ‚Üí `bizdev_source`
- Updated foreign keys and indexes

**‚ö†Ô∏è Breaking Change:** Application code must be updated

---

#### 052_tenant_identifiers.sql
**Date:** 2025-11-14  
**Purpose:** üö® **CRITICAL** - Enforce UUID-based tenant identification

**Changes:**
1. Changed `tenant_id` type to UUID on all tables
2. Removed `tenant_id` slug column (renamed to `slug`)
3. Updated RLS policies to use UUID
4. Updated foreign keys

**Migration Process:**
```sql
-- Step 1: Add new UUID column
ALTER TABLE contacts ADD COLUMN tenant_uuid UUID;

-- Step 2: Populate from tenant mapping
UPDATE contacts c SET tenant_uuid = t.id
FROM tenant t WHERE c.tenant_id = t.slug;

-- Step 3: Drop old column
ALTER TABLE contacts DROP COLUMN tenant_id;

-- Step 4: Rename new column
ALTER TABLE contacts RENAME COLUMN tenant_uuid TO tenant_id;

-- Step 5: Add foreign key
ALTER TABLE contacts 
  ADD CONSTRAINT fk_contacts_tenant 
  FOREIGN KEY (tenant_id) REFERENCES tenant(id) ON DELETE CASCADE;
```

**Impact:**
- ‚úÖ Proper foreign key relationships
- ‚úÖ Better performance (UUID indexing)
- ‚úÖ No more slug ‚Üí UUID conversions in application
- ‚ö†Ô∏è Breaking change for API consumers

### Tenant mapping and UUID-first guidance

After `052_tenant_identifiers.sql` the system requires tenant UUIDs for all queries and API calls. The legacy slug value was preserved on the `tenant.slug` column during migration.

- To look up a tenant UUID from a known slug:

```sql
SELECT id AS tenant_id
FROM tenant
WHERE slug = 'acme-corp'
LIMIT 1;
```

- To find the slug for a given UUID:

```sql
SELECT slug
FROM tenant
WHERE id = '550e8400-e29b-41d4-a716-446655440000'::uuid
LIMIT 1;
```

- Best practices:
  - Always use the tenant UUID (`tenant.id`) in API requests and internal queries: `tenant_id = '<uuid>'`.
  - If your client shows a friendly name, keep the slug only for UI display and map it to the UUID server-side.
  - Avoid storing or passing the slug as `tenant_id` ‚Äî do not rely on slugs for access control or RLS.
  - Use the `tenant` table as the single source of truth for mapping (slug ‚Üî id).

If you need to support legacy clients that still send slugs, implement a server-side translation layer that resolves the slug to the UUID and rejects ambiguous or missing mappings with a clear error (HTTP 400). Example Node snippet:

```javascript
// Resolve tenant slug to UUID (pseudo-code)
async function resolveTenantId(slug, db) {
  const res = await db.query('SELECT id FROM tenant WHERE slug = $1 LIMIT 1', [slug]);
  if (!res.rows.length) throw new Error('Unknown tenant slug');
  return res.rows[0].id;
}
```

#### 075_agent_memory_archive.sql
**Date:** 2025-11-16  
**Purpose:** Introduce long-term archival tables for ephemeral agent memory.

**Tables Created:**
- `agent_sessions_archive` ‚Äì Archived session metadata and payload
- `agent_events_archive` ‚Äì Archived individual agent events

**Features:**
- JSONB payload storage for flexible schema evolution
- Service-role controlled (RLS enabled, only backend writes)
- Indexed by `tenant_id`, `user_id`, `session_id`, `created_at` for efficient querying

#### 076_agent_sessions_archive_unique.sql
**Date:** 2025-11-16  
**Purpose:** Enforce uniqueness and idempotent archival.

**Actions:**
1. Deduplicate existing rows keeping earliest per `(tenant_id, user_id, session_id)`.
2. Add unique constraint `agent_sessions_archive_unique`.
3. Update archival job to use `UPSERT` on conflict for idempotent writes.

**Operational Impact:**
- Multiple archive attempts for the same session do not create duplicates.
- Prevents compliance/audit tooling from miscounting session volume.

### Canonical Tenant Resolution & Caching
To support UUID-first enforcement while allowing legacy identifiers, a canonical resolver normalizes inputs and adds light caching:

**Backend Module:** `backend/lib/tenantCanonicalResolver.js`

**Endpoints:**
- Single resolve: `GET /api/tenantresolve/:identifier?stats=true`
- Batch resolve: `GET /api/tenantresolve?ids=a,b,c&stats=true`
- Reset cache: `POST /api/tenantresolve/reset`
- Prometheus metrics: `GET /api/tenantresolve/metrics`

**Response Fields:**
- `uuid` ‚Äì Canonical tenant UUID (or null if unresolved)
- `slug` ‚Äì Display-friendly slug (or original input if unknown)
- `found` ‚Äì Boolean indicating successful DB lookup
- `source` ‚Äì Resolution path (`env`, `db-id`, `db-slug`, `uuid-input`, etc.), with `-cache` suffix for cached responses
- `cache` ‚Äì (Optional, when `?stats=true`) Object containing `{ ttlMs, size, hits, misses, hitRatio }`

**Caching:** In-memory TTL (default 60s via `TENANT_RESOLVE_CACHE_TTL_MS`) reduces repeat Supabase lookups under load.

**Cache Instrumentation:**
- **Hit/Miss Counters:** Track cache efficiency with global counters (`_cacheHits`, `_cacheMisses`)
- **Stats Export:** Append `?stats=true` to resolve endpoints to include cache metrics in JSON response
- **Prometheus Metrics:** Visit `/api/tenantresolve/metrics` for scraper-compatible text format:
  - `tenant_resolve_cache_size` (gauge) - Current cache entries
  - `tenant_resolve_cache_hits_total` (counter) - Cumulative hits
  - `tenant_resolve_cache_misses_total` (counter) - Cumulative misses
  - `tenant_resolve_cache_hit_ratio` (gauge) - Hit ratio 0-1
  - `tenant_resolve_cache_ttl_ms` (gauge) - TTL configuration
- **Cache Reset:** `POST /api/tenantresolve/reset` clears all cached entries and resets hit/miss counters (useful after tenant schema changes or for testing)

**System Tenant:** Set `SYSTEM_TENANT_ID` in `backend/.env` so `'system'` always resolves to a stable UUID.

**Archival Provenance:** Archived session/event JSON includes `_tenant` object for auditing:
```json
{
  "_tenant": {
    "input": "system",
    "slug": "system",
    "uuid": "a11dfb63-4b18-4eb8-872e-747af2e37c46",
    "source": "env"
  }
}
```

**Best Practices:**
1. Resolve tenant once at session start; reuse UUID.
2. Reject ambiguous or missing identifiers early (HTTP 400).
3. Never perform authorization logic on slugs; always use UUID.
4. Monitor cache hit ratio via `/api/tenantresolve/metrics` and tune `TENANT_RESOLVE_CACHE_TTL_MS` based on workload.
5. Use `POST /api/tenantresolve/reset` after bulk tenant operations to ensure fresh data.
6. Integrate `/api/tenantresolve/metrics` with your Prometheus/Grafana stack for continuous monitoring.


---

### 7.5 Special Migrations

#### 999_enable_rls_policies.sql
**Date:** 2025-11-14  
**Purpose:** Final RLS enforcement across all tables

**Actions:**
```sql
-- Force RLS even for table owner
ALTER TABLE contacts FORCE ROW LEVEL SECURITY;
ALTER TABLE accounts FORCE ROW LEVEL SECURITY;
-- ... 48 more tables

-- Verify RLS enabled
SELECT tablename, rowsecurity 
FROM pg_tables 
WHERE schemaname = 'public' AND rowsecurity = false;
-- Should return no rows
```

---

#### ROLLBACK_050_flatten_tenant_metadata.sql
**Purpose:** Rollback migration 050 if needed

**Process:**
1. Restore fields to metadata JSONB
2. Drop new columns
3. Verify data integrity

---

#### FIX_deactivate_test_admins.sql
**Purpose:** Deactivate test users in production

```sql
UPDATE users 
SET status = 'inactive' 
WHERE email LIKE '%test%' OR email LIKE '%@example.com';
```

---

#### DIAGNOSTIC_check_users.sql
**Purpose:** Verify user data integrity

```sql
-- Check for duplicate emails
SELECT email, COUNT(*) 
FROM users 
GROUP BY email 
HAVING COUNT(*) > 1;

-- Check for orphaned tenant references
SELECT u.email, u.tenant_id 
FROM users u 
LEFT JOIN tenant t ON u.tenant_id = t.id 
WHERE u.tenant_id IS NOT NULL AND t.id IS NULL;

-- Check role distribution
SELECT role, COUNT(*) FROM users GROUP BY role;
```

---

## Chapter 8: Backup & Recovery

### 8.1 Backup Strategy

#### Backup Types

| Type | Frequency | Retention | Purpose |
|------|-----------|-----------|---------|
| **Automated Daily** | Daily 3am UTC | 7 days (Free) / 30 days (Pro) | Point-in-time recovery |
| **Weekly Full** | Sunday 2am UTC | 4 weeks | Long-term recovery |
| **Pre-Migration** | Before each migration | 7 days | Rollback safety |
| **On-Demand** | As needed | 7 days | Before major changes |

---

### 8.2 Automated Backups

Supabase automatically backs up your database:

#### Free Tier
- **Frequency:** Daily
- **Retention:** 7 days
- **Recovery Window:** Up to 7 days ago

#### Pro Tier
- **Frequency:** Daily + hourly snapshots
- **Retention:** 30 days
- **Point-in-Time Recovery:** Any second within 30 days

#### Accessing Backups

1. **Supabase Dashboard**
   - Navigate to **Settings ‚Üí Database**
   - Click **Backups** tab
   - View list of available backups

2. **Restore from Backup**
   - Select backup date/time
   - Click **Restore**
   - Confirm restoration (‚ö†Ô∏è WILL OVERWRITE current data)
   - Wait 5-15 minutes for completion

---

### 8.3 Manual Backups

#### Full Database Backup (pg_dump)

```bash
# Backup entire database
pg_dump "postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" \
  --format=custom \
  --file=backup_$(date +%Y%m%d_%H%M%S).dump

# Backup specific tables
pg_dump "postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" \
  --format=custom \
  --table=users \
  --table=employees \
  --table=accounts \
  --file=critical_tables_backup.dump

# Backup schema only (no data)
pg_dump "postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" \
  --schema-only \
  --file=schema_only.sql
```

#### Restore from pg_dump

```bash
# Restore entire database
pg_restore --clean --if-exists \
  --dbname="postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" \
  backup_20251115_143022.dump

# Restore specific table
pg_restore --table=users \
  --dbname="postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" \
  backup_20251115_143022.dump
```

---

### 8.4 Point-in-Time Recovery

**Pro Tier Feature:** Restore database to any second within retention period

#### Use Cases
- Accidental DELETE without WHERE clause
- Bulk UPDATE with incorrect values
- User error (dropped table)
- Data corruption

#### Recovery Process

1. **Identify Target Time**
   ```sql
   -- Check audit_log for problematic change
   SELECT * FROM audit_log 
   WHERE action = 'DELETE' 
     AND table_name = 'contacts'
     AND changed_at > '2025-11-15 10:00:00'
   ORDER BY changed_at DESC
   LIMIT 10;
   ```

2. **Supabase Dashboard**
   - Settings ‚Üí Database ‚Üí Backups
   - Click **Point-in-Time Recovery**
   - Enter date/time: `2025-11-15 10:05:00 UTC`
   - Review impact (will overwrite current data)
   - Click **Restore**

3. **Verify Recovery**
   ```sql
   -- Check if data restored
   SELECT COUNT(*) FROM contacts WHERE created_at < '2025-11-15 10:05:00';
   ```

‚ö†Ô∏è **WARNING:** PITR restores entire database, not individual tables

---

### 8.5 Disaster Recovery

#### Disaster Scenarios

| Scenario | Recovery Method | RTO | RPO |
|----------|----------------|-----|-----|
| **Accidental DELETE** | Point-in-Time Recovery | 15 min | Seconds |
| **Corrupted Data** | Restore from backup | 30 min | 24 hours |
| **Dropped Table** | PITR or manual backup | 15 min | Seconds |
| **Supabase Outage** | Wait for restoration | Varies | 0 (replicated) |
| **Region Failure** | Restore to new region | 2-4 hours | 24 hours |

#### Disaster Recovery Plan

**Step 1: Assess Damage**
```sql
-- Check table existence
SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';

-- Check row counts
SELECT 
  schemaname,
  tablename,
  n_live_tup AS row_count
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC;
```

**Step 2: Determine Recovery Strategy**
- If < 1 hour ago ‚Üí PITR
- If > 1 hour ago ‚Üí Daily backup
- If schema corruption ‚Üí Manual backup + migration replay

**Step 3: Execute Recovery**

**Step 4: Verify Data Integrity**
```sql
-- Run diagnostic queries
SELECT COUNT(*) FROM users;
SELECT COUNT(*) FROM employees;
SELECT COUNT(*) FROM accounts;
SELECT COUNT(*) FROM contacts;

-- Check for orphaned records
SELECT c.id, c.email FROM contacts c
LEFT JOIN accounts a ON c.account_id = a.id
WHERE c.account_id IS NOT NULL AND a.id IS NULL;
```

**Step 5: Document Incident**
- Record time of incident
- Record recovery method used
- Document data loss (if any)
- Update runbook with lessons learned

---

## Chapter 9: Performance Monitoring

### 9.1 Supabase Dashboard

#### Key Metrics

**Database ‚Üí Reports**
1. **Database Health**
   - CPU usage (should be < 70%)
   - Memory usage (should be < 80%)
   - Disk usage (alert at 80%)
   - Connection count (monitor vs. max)

2. **Query Performance**
   - Slowest queries (> 100ms)
   - Most frequent queries
   - Query errors

3. **API Performance**
   - Requests per second
   - Average response time
   - Error rate

---

### 9.2 Query Performance

#### pg_stat_statements

```sql
-- Enable extension (if not already)
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Top 10 slowest queries
SELECT
  query,
  calls,
  total_exec_time,
  mean_exec_time,
  max_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Most frequently called queries
SELECT
  query,
  calls,
  total_exec_time,
  mean_exec_time
FROM pg_stat_statements
ORDER BY calls DESC
LIMIT 10;
```

#### EXPLAIN ANALYZE

```sql
EXPLAIN ANALYZE
SELECT c.*, a.name AS account_name
FROM contacts c
LEFT JOIN accounts a ON c.account_id = a.id
WHERE c.tenant_id = 'tenant-uuid'
ORDER BY c.created_at DESC
LIMIT 50;

-- Look for:
-- - Sequential Scan (bad) ‚Üí add index
-- - Index Scan (good)
-- - High execution time ‚Üí optimize query
```

---

### 9.3 Connection Pooling

#### Monitoring Connections

```sql
-- Current connections
SELECT 
  datname,
  usename,
  application_name,
  client_addr,
  state,
  query
FROM pg_stat_activity
WHERE datname = 'postgres';

-- Connection counts by state
SELECT state, COUNT(*) 
FROM pg_stat_activity 
WHERE datname = 'postgres'
GROUP BY state;
```

#### Connection Pool Configuration

```javascript
// Backend configuration
const { Pool } = require('pg');

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,                    // Maximum connections
  idleTimeoutMillis: 30000,   // Close idle after 30s
  connectionTimeoutMillis: 2000
});

// Monitor pool
pool.on('acquire', () => {
  console.log('Connection acquired from pool');
});

pool.on('remove', () => {
  console.log('Connection removed from pool');
});
```

---

### 9.4 Slow Query Detection

#### Log Slow Queries

```sql
-- Set threshold (milliseconds)
ALTER DATABASE postgres SET log_min_duration_statement = 100;

-- Check current setting
SHOW log_min_duration_statement;
```

#### Identify Slow Queries

```sql
-- Queries taking > 1 second on average
SELECT
  substring(query, 1, 100) AS short_query,
  calls,
  total_exec_time / 1000 AS total_seconds,
  mean_exec_time / 1000 AS avg_seconds
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- > 1 second
ORDER BY mean_exec_time DESC
LIMIT 20;
```

---

### 9.5 Performance Optimization

#### Query Optimization Checklist

- ‚úÖ Index foreign key columns
- ‚úÖ Index WHERE clause columns
- ‚úÖ Index ORDER BY columns
- ‚úÖ Use composite indexes for common patterns
- ‚úÖ Avoid SELECT * (specify columns)
- ‚úÖ Use LIMIT for pagination
- ‚úÖ Use prepared statements
- ‚úÖ Batch INSERT operations

#### Example: Optimize Contact Search

**Before (Slow):**
```sql
SELECT * FROM contacts 
WHERE LOWER(email) LIKE '%@example.com%';
-- Sequential scan, no index usage
```

**After (Fast):**
```sql
-- Create GIN index for pattern matching
CREATE INDEX idx_contacts_email_gin ON contacts USING GIN (email gin_trgm_ops);

-- Use index
SELECT id, first_name, last_name, email 
FROM contacts 
WHERE email ILIKE '%@example.com%';
```

---

## Chapter 10: Maintenance Tasks

### 10.1 Routine Maintenance

#### Weekly Tasks
- Review slow query log
- Check disk usage
- Monitor connection pool
- Review error logs

#### Monthly Tasks
- VACUUM ANALYZE all tables
- Reindex frequently updated tables
- Review and remove unused indexes
- Update table statistics

#### Quarterly Tasks
- Full database backup to external storage
- Security audit
- Performance baseline comparison
- Review and optimize queries

---

### 10.2 Vacuum & Analyze

#### What is VACUUM?

PostgreSQL uses MVCC (Multi-Version Concurrency Control), creating dead tuples. VACUUM reclaims space.

#### Manual VACUUM

```sql
-- Vacuum single table
VACUUM contacts;

-- Vacuum with analyze (update statistics)
VACUUM ANALYZE contacts;

-- Vacuum entire database
VACUUM ANALYZE;

-- Aggressive vacuum (slower, more thorough)
VACUUM FULL contacts;
```

#### Auto-Vacuum

Supabase enables auto-vacuum by default:

```sql
-- Check auto-vacuum settings
SELECT name, setting FROM pg_settings 
WHERE name LIKE 'autovacuum%';

-- Check last vacuum time
SELECT 
  schemaname,
  tablename,
  last_vacuum,
  last_autovacuum,
  n_dead_tup
FROM pg_stat_user_tables
ORDER BY n_dead_tup DESC;
```

---

### 10.3 Index Maintenance

#### Identify Missing Indexes

```sql
-- Tables with sequential scans
SELECT 
  schemaname,
  tablename,
  seq_scan,
  idx_scan,
  seq_scan / NULLIF(idx_scan, 0) AS scan_ratio
FROM pg_stat_user_tables
WHERE seq_scan > 1000
ORDER BY seq_scan DESC;

-- Foreign keys without indexes
SELECT
  tc.table_name,
  kcu.column_name
FROM information_schema.table_constraints tc
JOIN information_schema.key_column_usage kcu 
  ON tc.constraint_name = kcu.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND NOT EXISTS (
    SELECT 1 FROM pg_indexes 
    WHERE tablename = tc.table_name 
      AND indexdef LIKE '%' || kcu.column_name || '%'
  );
```

---

### 10.4 Table Bloat

#### Check Bloat

```sql
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
  n_live_tup,
  n_dead_tup,
  ROUND(n_dead_tup::numeric / NULLIF(n_live_tup, 0) * 100, 2) AS bloat_percent
FROM pg_stat_user_tables
WHERE n_dead_tup > 1000
ORDER BY n_dead_tup DESC;
```

#### Fix Bloat

```sql
-- Light cleanup
VACUUM contacts;

-- Aggressive cleanup (locks table)
VACUUM FULL contacts;
```

---

### 10.5 Statistics Updates

```sql
-- Update statistics for query planner
ANALYZE contacts;

-- Update all tables
ANALYZE;

-- Check statistics staleness
SELECT 
  schemaname,
  tablename,
  last_analyze,
  last_autoanalyze,
  n_mod_since_analyze
FROM pg_stat_user_tables
WHERE n_mod_since_analyze > 1000
ORDER BY n_mod_since_analyze DESC;
```

---

## Chapter 11: Troubleshooting

### 11.1 Common Errors

#### "relation does not exist"

**Error:**
```
ERROR: relation "contacts" does not exist
```

**Causes:**
1. Table not created yet
2. Wrong database connection
3. Wrong schema (public vs. other)

**Solutions:**
```sql
-- Check if table exists
SELECT * FROM information_schema.tables WHERE table_name = 'contacts';

-- Check current database
SELECT current_database();

-- List all tables
\dt

-- Check connection string
SELECT current_setting('DATABASE_URL');
```

üö® **COMMON MISTAKE:** Checking local PostgreSQL when Supabase Cloud is the production database.

---

#### "duplicate key value violates unique constraint"

**Error:**
```
ERROR: duplicate key value violates unique constraint "users_email_unique"
DETAIL: Key (email)=(test@example.com) already exists.
```

**Solutions:**
```sql
-- Find existing record
SELECT * FROM users WHERE email = 'test@example.com';

-- Update instead of insert
UPDATE users SET first_name = 'John' WHERE email = 'test@example.com';

-- Use ON CONFLICT (upsert)
INSERT INTO users (email, first_name, last_name)
VALUES ('test@example.com', 'John', 'Doe')
ON CONFLICT (email) DO UPDATE SET
  first_name = EXCLUDED.first_name,
  last_name = EXCLUDED.last_name;
```

---

#### "permission denied for table"

**Error:**
```
ERROR: permission denied for table contacts
```

**Causes:**
1. RLS blocking access
2. Wrong user role
3. Missing policy

**Solutions:**
```sql
-- Check RLS status
SELECT tablename, rowsecurity FROM pg_tables WHERE tablename = 'contacts';

-- Check policies
SELECT * FROM pg_policies WHERE tablename = 'contacts';

-- Temporarily disable RLS (testing only)
ALTER TABLE contacts DISABLE ROW LEVEL SECURITY;

-- Use service role key (backend)
-- Automatically bypasses RLS
```

---

### 11.2 Connection Issues

#### "too many connections"

**Error:**
```
FATAL: remaining connection slots are reserved for non-replication superuser connections
```

**Solutions:**
```sql
-- Check current connections
SELECT COUNT(*) FROM pg_stat_activity;

-- Kill idle connections
SELECT pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE state = 'idle' AND state_change < NOW() - INTERVAL '10 minutes';

-- Increase max_connections (Supabase Pro)
-- Dashboard ‚Üí Settings ‚Üí Database ‚Üí Connection Limit
```

---

#### "connection timeout"

**Causes:**
1. Network issues
2. Firewall blocking port 5432
3. Incorrect connection string
4. Supabase project paused (Free tier inactivity)

**Solutions:**
```bash
# Test connection
psql "postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres" -c "SELECT 1;"

# Check Supabase status
curl https://status.supabase.com/api/v2/status.json

# Verify connection string
echo $DATABASE_URL
```

---

### 11.3 Migration Failures

#### "column already exists"

**Solution:** Make migration idempotent

```sql
-- Bad
ALTER TABLE contacts ADD COLUMN phone VARCHAR(50);

-- Good
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns 
    WHERE table_name = 'contacts' AND column_name = 'phone'
  ) THEN
    ALTER TABLE contacts ADD COLUMN phone VARCHAR(50);
  END IF;
END $$;
```

---

#### "constraint already exists"

```sql
-- Check if constraint exists before adding
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint WHERE conname = 'contacts_email_unique'
  ) THEN
    ALTER TABLE contacts ADD CONSTRAINT contacts_email_unique UNIQUE (email);
  END IF;
END $$;
```

---

### 11.4 RLS Debugging

#### Test RLS Policy

```sql
-- Set tenant context
SET app.current_tenant_id = 'tenant-uuid';

-- Test query
SELECT * FROM contacts;  -- Should only return tenant's contacts

-- Reset context
RESET app.current_tenant_id;
```

#### Bypass RLS (Testing)

```sql
-- As superuser
ALTER TABLE contacts DISABLE ROW LEVEL SECURITY;

-- Test query
SELECT COUNT(*) FROM contacts;  -- Should return all contacts

-- Re-enable
ALTER TABLE contacts ENABLE ROW LEVEL SECURITY;
```

#### Check Policy Logic

```sql
-- View policy definition
SELECT 
  schemaname,
  tablename,
  policyname,
  permissive,
  roles,
  cmd,
  qual,
  with_check
FROM pg_policies
WHERE tablename = 'contacts';
```

---

### 11.5 Data Inconsistencies

#### Find Orphaned Records

```sql
-- Contacts without accounts
SELECT c.id, c.email, c.account_id
FROM contacts c
LEFT JOIN accounts a ON c.account_id = a.id
WHERE c.account_id IS NOT NULL AND a.id IS NULL;

-- Opportunities without contacts
SELECT o.id, o.name, o.contact_id
FROM opportunities o
LEFT JOIN contacts c ON o.contact_id = c.id
WHERE o.contact_id IS NOT NULL AND c.id IS NULL;
```

#### Fix Orphaned Records

```sql
-- Option 1: Set FK to NULL
UPDATE contacts SET account_id = NULL 
WHERE account_id NOT IN (SELECT id FROM accounts);

-- Option 2: Delete orphaned records
DELETE FROM contacts 
WHERE account_id NOT IN (SELECT id FROM accounts);
```

---

## Chapter 12: Best Practices

### 12.1 Development Workflow

#### Local Development Setup

1. **Use Supabase Project for Dev**
   - Create separate `aishacrm-dev` project
   - Use different DATABASE_URL
   - Apply migrations in dev first

2. **Migration Development**
   ```bash
   # Create migration
   touch backend/migrations/053_new_feature.sql
   
   # Test in dev
   psql $DEV_DATABASE_URL < backend/migrations/053_new_feature.sql
   
   # Verify
   psql $DEV_DATABASE_URL -c "\d new_table"
   
   # Apply to production (after testing)
   psql $PROD_DATABASE_URL < backend/migrations/053_new_feature.sql
   ```

3. **Version Control**
   - Commit migrations to git
   - Use descriptive commit messages
   - Document breaking changes in migration file

---

### 12.2 Testing Migrations

#### Pre-Production Testing

```sql
-- 1. Backup production
pg_dump $PROD_DATABASE_URL > prod_backup.sql

-- 2. Restore to dev
psql $DEV_DATABASE_URL < prod_backup.sql

-- 3. Apply migration to dev
psql $DEV_DATABASE_URL < backend/migrations/053_new_feature.sql

-- 4. Run application tests
npm test

-- 5. If successful, apply to production
psql $PROD_DATABASE_URL < backend/migrations/053_new_feature.sql
```

---

### 12.3 Documentation Standards

#### Migration File Headers

```sql
-- ============================================
-- Migration: 053_add_task_management.sql
-- Description: Add task tracking tables for project management
-- Date: 2025-11-15
-- Author: John Doe <john@example.com>
-- Dependencies: 052_tenant_identifiers.sql
-- Rollback: ROLLBACK_053_add_task_management.sql
-- ============================================

-- Breaking Changes:
-- - None

-- New Tables:
-- - tasks: Task definitions
-- - task_assignments: User assignments

-- Modified Tables:
-- - None

-- Indexes Added:
-- - idx_tasks_tenant
-- - idx_tasks_assigned_to
```

---

### 12.4 Security Considerations

#### Checklist

- ‚úÖ Enable RLS on all tenant-scoped tables
- ‚úÖ Create tenant isolation policies
- ‚úÖ Use `FORCE ROW LEVEL SECURITY`
- ‚úÖ Set function `search_path` explicitly
- ‚úÖ Never expose service role key in frontend
- ‚úÖ Use prepared statements (prevent SQL injection)
- ‚úÖ Validate input before database operations
- ‚úÖ Log sensitive operations to audit_log
- ‚úÖ Rotate database passwords quarterly
- ‚úÖ Use read-only roles for reporting tools

---

### 12.5 Scaling Strategies

#### Vertical Scaling (Supabase)

| Tier | vCPU | RAM | Storage | Max Connections |
|------|------|-----|---------|----------------|
| **Free** | Shared | 500MB | 500MB | 60 |
| **Pro** | 2 vCPU | 1GB | 8GB | 100 |
| **Team** | 4 vCPU | 4GB | 100GB | 200 |
| **Enterprise** | Custom | Custom | Custom | Custom |

#### Horizontal Scaling (Read Replicas)

**Pro Tier and Above:**
```javascript
// Write to primary
const { data, error } = await supabase
  .from('contacts')
  .insert({ name: 'John Doe' });

// Read from replica
const { data, error } = await supabase
  .from('contacts')
  .select('*')
  .limit(100);
// Automatically routed to read replica
```

#### Partitioning (Large Tables)

```sql
-- Partition activities by year
CREATE TABLE activities_2025 PARTITION OF activities
  FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

CREATE TABLE activities_2026 PARTITION OF activities
  FOR VALUES FROM ('2026-01-01') TO ('2027-01-01');
```

---

## Appendix A: Complete Table Reference

### Core CRM Tables

| Table | Description | Key Columns | Indexes |
|-------|-------------|-------------|---------|
| **accounts** | Companies/organizations | id, tenant_id, name, industry, website | tenant, name |
| **contacts** | Individual people | id, tenant_id, first_name, last_name, email, account_id | tenant, email, account |
| **leads** | Potential customers | id, tenant_id, first_name, last_name, email, company, status | tenant, status |
| **opportunities** | Sales deals | id, tenant_id, name, stage, amount, probability, account_id, contact_id | tenant, stage, account, contact |
| **activities** | Tasks/calls/emails | id, tenant_id, type, subject, status, related_id | tenant, related_id, status |

### User Management Tables

| Table | Description | Key Columns | Indexes |
|-------|-------------|-------------|---------|
| **users** | Global superadmins | id, email, password_hash, role | email, role |
| **employees** | Tenant-scoped users | id, tenant_id, email, first_name, last_name, role | tenant, email |
| **tenant** | Organizations | id, name, slug, status, plan | slug, status |

### System Tables

| Table | Description | Key Columns | Indexes |
|-------|-------------|-------------|---------|
| **audit_log** | Change tracking | id, action, table_name, record_id, changed_by, changed_at | changed_at, table_name |
| **system_logs** | Application logs | id, tenant_id, level, message, source | tenant, level, created_at |
| **performance_logs** | API performance | id, endpoint, duration, status_code | endpoint, created_at |
| **api_key** | API authentication | id, tenant_id, key_hash, expires_at | tenant |

(See complete list in Supabase Table Editor)

### AI Campaigns Table

The `ai_campaigns` table stores campaign definitions with flexible JSONB fields to support multiple channels (calls, emails, sequences).

- Columns:
  - `id UUID` ‚Äî Primary key
  - `tenant_id TEXT` ‚Äî Tenant UUID (string type in current table; resolved to UUID in app layer)
  - `name TEXT` ‚Äî Campaign name
  - `status TEXT` ‚Äî `draft|scheduled|running|paused|completed|cancelled`
  - `description TEXT` ‚Äî Optional description
  - `target_contacts JSONB` ‚Äî Array of recipients with scheduling and status fields
    - Example item keys: `contact_id`, `contact_name`, `phone` or `email`, `company`, `scheduled_date`, `scheduled_time`, `status`, `outcome`
  - `performance_metrics JSONB` ‚Äî Aggregates per channel (keys are extensible)
    - Calls: `total_calls`, `successful_calls`, `failed_calls`, `average_duration`, `appointments_set`, `leads_qualified`
    - Emails (when used): suggested keys include `total_emails`, `delivered`, `opened`, `clicked`, `bounced`
  - `metadata JSONB` ‚Äî Channel-specific configuration and advanced options
    - `campaign_type` ‚Äî `call|email|sequence` (default `call`)
    - `ai_provider` ‚Äî For call campaigns (e.g., `callfluent`, `thoughtly`)
    - `ai_call_integration_id` ‚Äî ID of tenant-owned call provider/agent integration (CallFluent/Thoughtly), required for call execution
    - `call_objective` ‚Äî Purpose for call campaigns (e.g., `follow_up`, `qualification`)
    - `call_settings` ‚Äî Object: `{ max_duration, retry_attempts, business_hours_only, timezone, delay_between_calls }`
    - `schedule_config` ‚Äî Object: `{ start_date, end_date, preferred_hours: { start, end }, excluded_days: [] }`
    - `ai_email_config` ‚Äî Object for email campaigns: `{ subject, body_template, sending_profile_id }` (sending profile MUST belong to the same tenant)
  - `created_at/updated_at TIMESTAMPTZ` ‚Äî Timestamps (updated via trigger)

Notes:
- The application persists channel-specific settings under `metadata` to keep the table narrow and extensible.
- `target_contacts` items include either `phone` (calls) or `email` (emails) based on `campaign_type`.
- Webhooks and external automation (e.g., n8n) can consume lifecycle events emitted by the backend layer; store configuration pointers under `metadata` as needed (e.g., `webhook_url`).

Tenant Enforcement:
- All campaign operations are strictly tenant-scoped. The UI only exposes integrations where `tenant_id` equals the current session‚Äôs tenant and `is_active = true`.
- Email campaigns must reference a tenant-owned sending profile via `ai_email_config.sending_profile_id`.
- Call campaigns must reference a tenant-owned provider/agent via `ai_call_integration_id`.
- Backend validates that referenced IDs belong to the same `tenant_id`; cross-tenant references are rejected with 403/404.

Campaign Execution Endpoints
- `POST /api/aicampaigns/:id/start` ‚Äî Queues a campaign for execution. Body must include `tenant_id`. Server validates:
  - Email: `metadata.ai_email_config.sending_profile_id` exists and belongs to `tenant_id` and is active in `tenant_integrations`.
  - Call: `metadata.ai_call_integration_id` exists and belongs to `tenant_id` and is active.
  - On success: sets `status='scheduled'`, appends `metadata.lifecycle.scheduled_at`, `scheduled_by`.
  - Future work: background worker enqueue + optional n8n/webhook lifecycle events.
- `POST /api/aicampaigns/:id/pause` ‚Äî Pauses a scheduled/running campaign. Body must include `tenant_id`. Sets `status='paused'` and stamps `metadata.lifecycle.paused_at`, `paused_by`.
- `POST /api/aicampaigns/:id/resume` ‚Äî Resumes a paused campaign. Body must include `tenant_id`. Sets `status='scheduled'` and stamps `metadata.lifecycle.resumed_at`, `resumed_by`.

Webhook Events (Optional)
- Enable by setting `WEBHOOKS_ENABLED=true` in backend env. Subscriptions are read from `webhook` table filtered by `tenant_id`, `is_active=true`, and `event_types` containing the event name or `*`.
- Emitted events: `aicampaign.created`, `aicampaign.start`, `aicampaign.pause`, `aicampaign.resume`, `aicampaign.progress`, `aicampaign.completed`, `aicampaign.failed`
- Delivery: JSON POST to subscriber `url` with optional `X-Webhook-Signature` header (HMAC-SHA256 over body using subscriber `secret`). Failures are non-blocking and not retried by default.
- See **System Admin Guide Chapter 9.5** for Campaign Worker configuration and monitoring.

Example insert (simplified):

```sql
INSERT INTO ai_campaigns (tenant_id, name, status, target_contacts, performance_metrics, metadata)
VALUES (
  '550e8400-e29b-41d4-a716-446655440000',
  'Q1 Nurture (Email)',
  'draft',
  '[{"contact_id":"...","contact_name":"Jane","email":"jane@example.com","status":"pending"}]',
  '{}',
  '{"campaign_type":"email","ai_email_config":{"subject":"Quick follow-up from {{company_name}}","body_template":"Hi {{contact_name}}, ..."},"schedule_config":{"start_date":"2025-12-01","preferred_hours":{"start":"09:00","end":"17:00"}}}'::jsonb
);
```

---

## Appendix B: Migration Quick Reference

| # | File | Description |
|---|------|-------------|
| 001 | init.sql | Initial schema with 8 core tables |
| 006 | tenant_table.sql | Multi-tenancy support |
| 014 | conversations.sql | AI conversation tracking |
| 020 | enforce_email_uniqueness.sql | Cross-table email validation |
| 023 | comprehensive_rls_security.sql | Full RLS implementation (CRITICAL) |
| 024 | fix_function_search_path.sql | Security fix for functions |
| 025 | add_foreign_key_indexes.sql | Performance optimization (30+ indexes) |
| 031 | create_ai_campaigns.sql | AI marketing campaigns |
| 033 | entity_lifecycle_with_id_preservation.sql | Lead/contact conversion |
| 050 | flatten_tenant_metadata.sql | Move tenant fields from JSONB to columns |
| 052 | tenant_identifiers.sql | UUID-based tenant IDs (BREAKING) |
| 999 | enable_rls_policies.sql | Force RLS on all tables |

---

## Appendix C: Supabase Features

### PostgREST API

Auto-generated REST API from database schema:

```javascript
// GET /rest/v1/contacts?tenant_id=eq.uuid
const { data, error } = await supabase
  .from('contacts')
  .select('*')
  .eq('tenant_id', 'uuid');

// POST /rest/v1/contacts
const { data, error } = await supabase
  .from('contacts')
  .insert({ name: 'John Doe' });
```

### Storage

```javascript
// Upload file
const { data, error } = await supabase.storage
  .from('documents')
  .upload('public/avatar.png', file);

// Get public URL
const { publicURL } = supabase.storage
  .from('documents')
  .getPublicUrl('public/avatar.png');
```

### Auth

```javascript
// Sign up
const { user, error } = await supabase.auth.signUp({
  email: 'user@example.com',
  password: 'password123'
});

// Sign in
const { user, error } = await supabase.auth.signIn({
  email: 'user@example.com',
  password: 'password123'
});
```

### Realtime

```javascript
// Subscribe to changes
const subscription = supabase
  .from('contacts')
  .on('INSERT', payload => {
    console.log('New contact:', payload.new);
  })
  .subscribe();
```

---

## Appendix D: SQL Query Examples

### Common Queries

#### Get Contact with Account Info
```sql
SELECT 
  c.id,
  c.first_name,
  c.last_name,
  c.email,
  a.name AS account_name,
  a.industry
FROM contacts c
LEFT JOIN accounts a ON c.account_id = a.id
WHERE c.tenant_id = 'tenant-uuid'
ORDER BY c.created_at DESC
LIMIT 50;
```

#### Sales Pipeline Summary
```sql
SELECT 
  stage,
  COUNT(*) AS count,
  SUM(amount) AS total_value,
  AVG(amount) AS avg_value,
  AVG(probability) AS avg_probability
FROM opportunities
WHERE tenant_id = 'tenant-uuid'
  AND stage != 'closed_lost'
GROUP BY stage
ORDER BY 
  CASE stage
    WHEN 'qualification' THEN 1
    WHEN 'proposal' THEN 2
    WHEN 'negotiation' THEN 3
    WHEN 'closed_won' THEN 4
  END;
```

#### Activity Report
```sql
SELECT 
  DATE(created_at) AS date,
  type,
  COUNT(*) AS count
FROM activities
WHERE tenant_id = 'tenant-uuid'
  AND created_at > NOW() - INTERVAL '30 days'
GROUP BY DATE(created_at), type
ORDER BY date DESC, type;
```

#### JSONB Queries
```sql
-- Find contacts with specific custom field
SELECT * FROM contacts
WHERE metadata->'custom_fields'->>'lead_source' = 'Website';

-- Update nested JSONB field
UPDATE contacts
SET metadata = jsonb_set(
  metadata,
  '{custom_fields,customer_tier}',
  '"Gold"'
)
WHERE id = 'contact-uuid';

-- Add new JSONB field
UPDATE contacts
SET metadata = metadata || '{"new_field": "value"}'::jsonb
WHERE id = 'contact-uuid';
```

---

## Appendix E: Troubleshooting Flowchart

```mermaid
graph TD
    A[Database Issue] --> B{Can connect?}
    B -->|No| C[Check connection string]
    B -->|Yes| D{Query failing?}
    
    C --> E[Verify DATABASE_URL]
    E --> F[Test with psql]
    F --> G[Check Supabase dashboard]
    
    D -->|Yes| H{Error message?}
    H -->|relation does not exist| I[Run migrations]
    H -->|permission denied| J[Check RLS policies]
    H -->|duplicate key| K[Check unique constraints]
    H -->|connection timeout| L[Check connection pool]
    
    I --> M[Apply missing migrations]
    J --> N[Verify tenant_id in query]
    K --> O[Use ON CONFLICT or check existing]
    L --> P[Increase pool size or kill idle]
    
    D -->|No| Q{Performance issue?}
    Q -->|Yes| R[Check slow query log]
    R --> S[Add indexes]
    R --> T[Optimize query]
    
    Q -->|No| U{Data issue?}
    U -->|Yes| V[Check for orphaned records]
    V --> W[Run data validation queries]
```

---

**[‚Üê Return to Part 1](./AISHA_CRM_DATABASE_MANUAL_PART1.md)** for Chapters 1-6

---

*Aisha CRM Database Administration Manual - Part 2*  
*Copyright ¬© 2025 4V Data Consulting. All rights reserved.*  
*Version 1.0 - November 15, 2025*
